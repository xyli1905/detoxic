{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_train_label():\n",
    "#     train_data_path = \"/Users/xyli1905/Projects/Datasets/Kaggle/QuoraToxicDetect/train.csv\"\n",
    "#     data = pd.read_csv(train_data_path)\n",
    "#     data_label = np.array(data[\"target\"])\n",
    "#     return data_label\n",
    "\n",
    "# def loadVocab():\n",
    "#     # load existed vocabulary\n",
    "#     vocab_path =  \"/Users/xyli1905/Projects/NLP/detoxic/data_proc/vocab.txt\"\n",
    "#     with open(vocab_path, 'r') as f:\n",
    "#         vocab = [word.split(\"\\n\")[0] for word in f.readlines()]\n",
    "#     return vocab\n",
    "\n",
    "# def loadTraintoken():\n",
    "#     # load tokenized training data\n",
    "#     train_token_path = \"/Users/xyli1905/Projects/NLP/detoxic/data_proc/traintoken.pkl\"\n",
    "#     with open(train_token_path, 'rb') as f:\n",
    "#         traintoken = pickle.load(f)\n",
    "#     return traintoken\n",
    "\n",
    "# def loadPretrainedWeight():\n",
    "#     # load pretrained embedding\n",
    "#     weight_data_path = \"/Users/xyli1905/Projects/NLP/detoxic/data_proc/pretrained_weight.npy\"\n",
    "#     pretrained_weight = np.load(weight_data_path)\n",
    "#     return pretrained_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define parameters of training data\n",
    "# sentence_cutoff = 60\n",
    "# embedding_dim = 300\n",
    "\n",
    "# def prepare_training_data():\n",
    "#     # load data\n",
    "#     vocab = loadVocab()\n",
    "#     vocab_len = len(vocab)\n",
    "#     idx_nothing = vocab_len\n",
    "    \n",
    "#     # load train token, builing train tensor with cutoff 60\n",
    "#     word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "    \n",
    "#     train_token = loadTraintoken()\n",
    "#     train_idx = np.full((len(train_token),sentence_cutoff), idx_nothing)\n",
    "#     for i, sentence in enumerate(train_token):\n",
    "#         for j, word in enumerate(sentence):\n",
    "#             if j >= sentence_cutoff:\n",
    "#                 break\n",
    "#             train_idx[i,j] = word_to_idx[word]\n",
    "#     train_tensor = torch.tensor(train_idx, dtype=torch.long)\n",
    "            \n",
    "#     # load training label\n",
    "#     labels = load_train_label()\n",
    "#     train_label = torch.from_numpy(labels).view(-1,1)\n",
    "    \n",
    "#     # set up pretrained embedding\n",
    "#     Pweight = loadPretrainedWeight()\n",
    "#     ## insert vector representing nothing at the end of pretrained embedding\n",
    "#     weight_nothing = np.zeros(embedding_dim)\n",
    "#     pretrained_weight = torch.from_numpy(np.vstack([Pweight, weight_nothing]))\n",
    "    \n",
    "#     # concatenate train_tensor & train_label to get full train data\n",
    "#     train_data = torch.cat((train_tensor, train_label), 1)\n",
    "\n",
    "#     return vocab_len, train_data, pretrained_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_len, train_data, pretrained_weight = prepare_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading preprocessed data\n",
    "# with open('../data_proc/processed_data/train_mat.pkl', 'rb') as f:\n",
    "#     # train_mat.pkl is np.ndarray\n",
    "#     train_data = torch.from_numpy(pickle.load(f))\n",
    "# with open('../data_proc/processed_data/pretrained_weight.pkl', 'rb') as f:\n",
    "#     # pretrained_weight is np.ndarray\n",
    "#     pretrained_weight = torch.from_numpy(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_cutoff = train_data.shape[1] - 1\n",
    "# vocab_len = pretrained_weight.shape[0] - 2\n",
    "# embedding_dim = pretrained_weight.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "torch.Size([1306122, 61]) torch.Size([52309, 300])\n",
      "60 52307 300\n"
     ]
    }
   ],
   "source": [
    "# print(type(train_data), type(pretrained_weight))\n",
    "# print(train_data.shape, pretrained_weight.shape)\n",
    "# print(sentence_cutoff, vocab_len, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_dir = \"/Users/xyli1905/Projects/NLP/detoxic/checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_num = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define train class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Train:\n",
    "#     def __init__(self, model, max_epoch=5):\n",
    "#         self._chkp_dir = ckpt_dir\n",
    "#         # parameter\n",
    "#         self._max_epoch = max_epoch\n",
    "#         self._batch_size = 64\n",
    "        \n",
    "#         # data processing for training\n",
    "#         #self._dataset = train_data\n",
    "#         self._dataset_train = DataLoader(train_data[:-valid_num,:], batch_size=self._batch_size,\n",
    "#                                          shuffle=True, drop_last=True)\n",
    "\n",
    "#         # define model\n",
    "#         self._model = model\n",
    "#         self._lossfun = torch.nn.CrossEntropyLoss()\n",
    "#         self._optimizer = torch.optim.SGD(self._model.parameters(), lr=0.01)\n",
    "        \n",
    "#         # train model\n",
    "#         self._train()\n",
    "        \n",
    "#         # update training flag\n",
    "#         if self._model.trained:\n",
    "#             self._model.training_times += 1\n",
    "#         else:\n",
    "#             self._model.trained = True\n",
    "#             self._model.training_times = 1\n",
    "    \n",
    "#     def _train(self):\n",
    "#         # may initialize info of training here\n",
    "#         # e.g. self._total_step\n",
    "#         # ...\n",
    "        \n",
    "#         for i_epoch in range(self._max_epoch):\n",
    "#             epoch_time_start = time.time()\n",
    "            \n",
    "#             # train epoch\n",
    "#             print(\"Start epoch %d / %d, \\t at %s\" % (i_epoch+1, self._max_epoch, time.asctime()))\n",
    "#             self._train_epoch(i_epoch)\n",
    "        \n",
    "#             # save model after each epoch here\n",
    "#             if (i_epoch != 0) and (i_epoch % 5 == 0 or i_epoch == self._max_epoch):\n",
    "#                 idx = self._model.save()\n",
    "#                 self._save_opt(idx)\n",
    "            \n",
    "#             # training time for each epoch\n",
    "#             time_cost = time.time() - epoch_time_start\n",
    "#             print(\"End of epoch %d / %d \\t Time taken: %d sec (or % d min)\" % \\\n",
    "#                   (i_epoch+1, self._max_epoch, time_cost, time_cost / 60.))\n",
    "            \n",
    "#             # may updata learning rate here\n",
    "#             # if i_epoch > ...\n",
    "\n",
    "#     def _train_epoch(self, i_epoch):\n",
    "\n",
    "#         for i_train_batch, train_batch in enumerate(self._dataset_train):\n",
    "            \n",
    "#             # forward model\n",
    "#             y_pred = self._model.forward(train_batch[:, :-1])\n",
    "#             LRloss = self._lossfun(y_pred, train_batch[:, -1])\n",
    "            \n",
    "#             # backprop\n",
    "#             self._optimizer.zero_grad()\n",
    "#             LRloss.backward()\n",
    "#             self._optimizer.step()\n",
    "            \n",
    "#     def _save_opt(self, idx):\n",
    "#         '''\n",
    "#         idx is from model.save, equal to model.training_times\n",
    "#         save optimizer parameters\n",
    "#         '''\n",
    "#         fname = 'opt_{}_{}_id.pth'.format(self._model.name, str(idx))\n",
    "#         save_path = os.path.join(self._chkp_dir, fname)\n",
    "#         torch.save(self._optimizer.state_dict(), save_path)\n",
    "#         print(\"optimizer for %s saved at %s\" % (self._model.name, save_path))\n",
    "    \n",
    "#     def _load_opt(self, idx=-1):\n",
    "#         '''\n",
    "#         load optimizer with idx,\n",
    "#         if idx=-1 load the one with max idx\n",
    "#         '''\n",
    "#         if idx == -1:\n",
    "#             idx_num = 0\n",
    "#             for file in os.listdir(self._chkp_dir):\n",
    "#                 if file.startswith(\"opt\"):\n",
    "#                     idx_num = max(idx_num, int(file.split('_')[2]))\n",
    "#         else:\n",
    "#             idx_num = idx\n",
    "#         #print(\"loading epoch %d ...\" % idx_num)\n",
    "#         fname = 'opt_{}_{}_id.pth'.format(self._model.name, str(idx_num))\n",
    "#         file_path = os.path.join(self._chkp_dir, fname)\n",
    "#         self.load_state_dict(torch.load(file_path))\n",
    "#         print(\"optimizer for %s loaded from %s\" % (self._model.name, file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test train codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq = torch.tensor([[1,2,4],[3,5,1]],dtype=torch.long)\n",
    "# print(seq, seq.shape[0])\n",
    "\n",
    "# TW = Variable(torch.randn(10), requires_grad=True)\n",
    "# print(TW)\n",
    "\n",
    "# TE = torch.tensor([[[1,2,3,4,5],[5,4,3,2,1],[2,1,3,4,5]],[[1,2,3,4,5],[5,4,3,2,1],[2,1,3,4,5]]], dtype=torch.float)\n",
    "# print(TE, TE.shape)\n",
    "\n",
    "# print(TW[seq])\n",
    "\n",
    "# def weighted_embed(x, s):\n",
    "#     output = torch.zeros((s.shape[0], 1, x.shape[2]))\n",
    "#     ww = TW[s].view(-1, 1, 3)\n",
    "#     for i in range(s.shape[0]):\n",
    "#         output[i] = torch.mm(ww[i], x[i])\n",
    "#     return output\n",
    "\n",
    "# out = weighted_embed(TE, seq)\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define LR models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### this is the Bag of Word LR model\n",
    "# ###\n",
    "# class LogisticRegressionBoW(torch.nn.Module):\n",
    "#     def __init__(self, trainable_emb=False):\n",
    "#         super(LogisticRegressionBoW, self).__init__()\n",
    "#         self.name = \"LRBoW\"\n",
    "#         self.trained = False\n",
    "#         self.training_times = 0\n",
    "#         self._chkp_dir = ckpt_dir\n",
    "#         self._save_mark = 0\n",
    "        \n",
    "#         # define parameters\n",
    "#         self.vocab_size = vocab_len\n",
    "#         #self.hidden_size = 60\n",
    "#         self.output_size = 2 # two labels 0,1\n",
    "        \n",
    "#         # define layers\n",
    "#         self.W = torch.nn.Parameter(torch.randn(self.vocab_size+1, self.output_size),requires_grad=True)\n",
    "#         self.b = torch.nn.Parameter(torch.zeros(self.output_size),requires_grad=True)\n",
    "#         #self.linear = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "#         #self.sigmoid = torch.nn.Sigmoid()\n",
    "#         #\n",
    "#         # initialize weights of layers\n",
    "#         #self.init_weight()\n",
    "        \n",
    "# #     def init_weight():\n",
    "#         # with other layers defaultly initialized\n",
    "    \n",
    "#     def weighted_words(self, seq):\n",
    "#         '''\n",
    "#         note idx_seq must be a torch tensor of dtype torch.long\n",
    "#         N is batch size, 60 is the limit of sentence len\n",
    "#         below, W[idx_seq]:(N, 60), output:(N, 1, 300)\n",
    "#         '''\n",
    "#         output = torch.zeros((seq.shape[0], self.output_size))\n",
    "#         for i in range(seq.shape[0]):\n",
    "#             idxseq = seq[i]\n",
    "#             output[i] = torch.sum(self.W[idxseq],0) + self.b\n",
    "#         return output\n",
    "            \n",
    "#     def forward(self, idx_seq):\n",
    "#         '''\n",
    "#         note idx_seq must be a torch tensor of dtype torch.long\n",
    "#         N is batch size, 60 is the limit of sentence len\n",
    "#         '''\n",
    "#         out = self.weighted_words(idx_seq) # idx_seq:(N, 60), out:(N, 2)\n",
    "#         #out = self.linear(X).view(-1,self.output_size)\n",
    "#         return out\n",
    "    \n",
    "#     def predict(self, idx_seq):\n",
    "#         # 0: sincere; 1: toxic\n",
    "#         y = F.softmax(self.forward(idx_seq), dim=1)\n",
    "#         y_pred = torch.argmax(y, dim=1)\n",
    "#         return y_pred\n",
    "    \n",
    "#     def save(self):\n",
    "#         '''\n",
    "#         save trained parameters\n",
    "#         '''\n",
    "#         fname = 'net_LRBoW_{}_id.pth'.format(str(self._save_mark))\n",
    "#         self._save_mark += 1\n",
    "#         save_path = os.path.join(self._chkp_dir, fname)\n",
    "#         torch.save(self.state_dict(), save_path)\n",
    "#         print(\"LRBoW saved at %s\" % (save_path))\n",
    "#         return self._save_mark - 1\n",
    "        \n",
    "#     def load(self, idx=-1):\n",
    "#         '''\n",
    "#         load trained model with idx,\n",
    "#         if idx=-1 load the one with max idx\n",
    "#         '''\n",
    "#         if idx == -1:\n",
    "#             idx_num = 0\n",
    "#             for file in os.listdir(self._chkp_dir):\n",
    "#                 if file.startswith(\"net\"):\n",
    "#                     idx_num = max(idx_num, int(file.split('_')[2]))\n",
    "#         else:\n",
    "#             idx_num = idx\n",
    "#         #print(\"loading epoch %d ...\" % epoch_num)\n",
    "#         fname = 'net_LRBoW_{}_id.pth'.format(str(idx_num))\n",
    "#         self._save_mark = idx_num\n",
    "#         file_path = os.path.join(self._chkp_dir, fname)\n",
    "#         self.load_state_dict(torch.load(file_path))\n",
    "#         print(\"LRBoW loaded from %s\" % (file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = LogisticRegressionBoW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### this old LR model connot compute grad for W for some unknown reason, discarded for now\n",
    "# ###\n",
    "# class LogisticRegressionEmbBoW(torch.nn.Module):\n",
    "#     def __init__(self, trainable_emb=False):\n",
    "#         super(LogisticRegressionEmbBoW, self).__init__()\n",
    "#         self.name = \"LRembBoW\"\n",
    "#         self.trained = False\n",
    "#         self.training_times = 0\n",
    "#         self._chkp_dir = ckpt_dir\n",
    "#         self._save_mark = 0\n",
    "        \n",
    "#         # define parameters\n",
    "#         self.vocab_size = vocab_len\n",
    "#         self.emb_size = embedding_dim\n",
    "#         self.trainable_emb = trainable_emb\n",
    "#         self.hidden_size = 64\n",
    "#         self.output_size = 2 # two labels 0,1\n",
    "#         ##self-defined model parameter\n",
    "#         self.W = torch.nn.Parameter(torch.randn(self.vocab_size+1),requires_grad=True)\n",
    "        \n",
    "#         # define layers\n",
    "#         self.embed = torch.nn.Embedding(self.vocab_size+1, self.emb_size)\n",
    "#         self.linear1 = torch.nn.Linear(self.emb_size, self.hidden_size)\n",
    "#         self.linear2 = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "#         #self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "#         # initialize weights of layers\n",
    "#         self.init_weight()\n",
    "        \n",
    "#     def init_weight(self):\n",
    "#         self.embed.weight.data.copy_(pretrained_weight)\n",
    "#         self.embed.weight.requires_grad = self.trainable_emb\n",
    "#         # with other layers defaultly initialized\n",
    "    \n",
    "#     def weighted_embed(self, x, seq):\n",
    "#         '''\n",
    "#         note idx_seq must be a torch tensor of dtype torch.long\n",
    "#         N is batch size, 60 is the limit of sentence len\n",
    "#         below, W[idx_seq]:(N, 60), output:(N, 1, 300)\n",
    "#         '''\n",
    "#         output = torch.zeros((seq.shape[0], 1, x.shape[2]))\n",
    "#         for i in range(seq.shape[0]):\n",
    "#             idxseq = seq[i:i+1]\n",
    "#             output[i] = torch.mm(self.W[idxseq], x[i])\n",
    "#         return output\n",
    "            \n",
    "#     def forward(self, idx_seq):\n",
    "#         '''\n",
    "#         note idx_seq must be a torch tensor of dtype torch.long\n",
    "#         N is batch size, 60 is the limit of sentence len\n",
    "#         '''\n",
    "#         X = self.embed(idx_seq) # idx_seq:(N, 60), X:(N, 60, 300)\n",
    "#         X = self.weighted_embed(X, idx_seq)\n",
    "#         X = F.relu(self.linear1(X))\n",
    "#         out = self.linear2(X).view(-1,self.output_size)\n",
    "#         return out\n",
    "    \n",
    "#     def predict(self, idx_seq):\n",
    "#         # 0: sincere; 1: toxic\n",
    "#         y = F.softmax(self.forward(idx_seq), dim=1)\n",
    "#         y_pred = torch.argmax(y, dim=1)\n",
    "#         return y_pred\n",
    "    \n",
    "#     def save(self):\n",
    "#         '''\n",
    "#         save trained parameters\n",
    "#         '''\n",
    "#         fname = 'net_LRembBoW_{}_id.pth'.format(str(self._save_mark))\n",
    "#         self._save_mark += 1\n",
    "#         save_path = os.path.join(self._chkp_dir, fname)\n",
    "#         torch.save(self.state_dict(), save_path)\n",
    "#         print(\"LRembBoW saved at %s\" % (save_path))\n",
    "#         return self._save_mark - 1\n",
    "        \n",
    "#     def load(self, idx=-1):\n",
    "#         '''\n",
    "#         load trained model with idx,\n",
    "#         if idx=-1 load the one with max idx\n",
    "#         '''\n",
    "#         if idx == -1:\n",
    "#             idx_num = 0\n",
    "#             for file in os.listdir(self._chkp_dir):\n",
    "#                 if file.startswith(\"net\"):\n",
    "#                     idx_num = max(idx_num, int(file.split('_')[2]))\n",
    "#         else:\n",
    "#             idx_num = idx\n",
    "#         #print(\"loading epoch %d ...\" % epoch_num)\n",
    "#         fname = 'net_LRembBoW_{}_id.pth'.format(str(idx_num))\n",
    "#         self._save_mark = idx_num\n",
    "#         file_path = os.path.join(self._chkp_dir, fname)\n",
    "#         self.load_state_dict(torch.load(file_path))\n",
    "#         print(\"LRembBoW loaded from %s\" % (file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # note in this notebook, vocab & Pweight are global variables\n",
    "# # in this new version, we set W -> (1,300) namely use embeddings to identify words & assign them weights\n",
    "# class LogisticRegressionEmb(torch.nn.Module):\n",
    "#     def __init__(self, trainable_emb=False):\n",
    "#         super(LogisticRegressionEmb, self).__init__()\n",
    "#         self.name = \"LRemb\"\n",
    "#         self.trained = False\n",
    "#         self.training_times = 0\n",
    "#         self._chkp_dir = ckpt_dir\n",
    "#         self._save_mark = 0\n",
    "        \n",
    "#         # define parameters\n",
    "#         self.vocab_size = vocab_len\n",
    "#         self.emb_size = embedding_dim\n",
    "#         self.trainable_emb = trainable_emb\n",
    "#         self.hidden_size = 64\n",
    "#         self.output_size = 2 # two labels 0,1\n",
    "#         ##self-defined model parameter\n",
    "#         self.W = torch.nn.Parameter(torch.randn(embedding_dim, 1),requires_grad=True)\n",
    "        \n",
    "#         # define layers\n",
    "#         self.embed = torch.nn.Embedding(self.vocab_size+1, self.emb_size)\n",
    "#         self.linear1 = torch.nn.Linear(self.emb_size, self.hidden_size)\n",
    "#         self.linear2 = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "#         #self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "#         # initialize weights of layers\n",
    "#         self.init_weight()\n",
    "        \n",
    "#     def init_weight(self):\n",
    "#         self.embed.weight.data.copy_(pretrained_weight)\n",
    "#         self.embed.weight.requires_grad = self.trainable_emb\n",
    "#         # with other layers defaultly initialized\n",
    "\n",
    "#     def emb_selfcorr(self, x):\n",
    "#         '''\n",
    "#         basically do matmul(E.T, E), dim=(300, 300), kind of a self-correlation\n",
    "#         '''\n",
    "#         output = torch.zeros((x.shape[0], self.emb_size, self.emb_size))\n",
    "#         for i in range(x.shape[0]):\n",
    "#             output[i] = torch.mm(torch.t(x[i]), x[i])\n",
    "#         return output\n",
    "        \n",
    "#     def forward(self, idx_seq):\n",
    "#         '''\n",
    "#         note idx_seq must be a torch tensor of dtype torch.long\n",
    "#         N is batch size, 60: sentence_cutoff, 300: embedding_dim\n",
    "#         input idx_seq:(N, 60)\n",
    "#         '''\n",
    "#         X = self.embed(idx_seq)  # emb vec X:(N, 60, 300)\n",
    "#         X = self.emb_selfcorr(X) # give corr mat: (N, 300, 300)\n",
    "#         X = torch.squeeze(torch.matmul(X, self.W))   # weighted sum of embeddings (N, 300)\n",
    "#         X = F.relu(self.linear1(X))\n",
    "#         out = self.linear2(X).view(-1,self.output_size)\n",
    "#         return out\n",
    "    \n",
    "#     def predict(self, idx_seq):\n",
    "#         '''\n",
    "#         0: sincere; 1: toxic\n",
    "#         '''\n",
    "#         y = F.softmax(self.forward(idx_seq), dim=1)\n",
    "#         y_pred = torch.argmax(y, dim=1)\n",
    "#         return y_pred\n",
    "    \n",
    "#     def save(self):\n",
    "#         '''\n",
    "#         save trained parameters\n",
    "#         '''\n",
    "#         fname = 'net_LRemb_{}_id.pth'.format(str(self._save_mark))\n",
    "#         self._save_mark += 1\n",
    "#         save_path = os.path.join(self._chkp_dir, fname)\n",
    "#         torch.save(self.state_dict(), save_path)\n",
    "#         print(\"LRemb saved at %s\" % (save_path))\n",
    "#         return self._save_mark - 1\n",
    "        \n",
    "#     def load(self, idx=-1):\n",
    "#         '''\n",
    "#         load trained model with idx,\n",
    "#         if idx=-1 load the one with max idx\n",
    "#         '''\n",
    "#         if idx == -1:\n",
    "#             idx_num = 0\n",
    "#             for file in os.listdir(self._chkp_dir):\n",
    "#                 if file.startswith(\"net\"):\n",
    "#                     idx_num = max(idx_num, int(file.split('_')[2]))\n",
    "#         else:\n",
    "#             idx_num = idx\n",
    "#         #print(\"loading epoch %d ...\" % epoch_num)\n",
    "#         fname = 'net_LRemb_{}_id.pth'.format(str(idx_num))\n",
    "#         self._save_mark = idx_num\n",
    "#         file_path = os.path.join(self._chkp_dir, fname)\n",
    "#         self.load_state_dict(torch.load(file_path))\n",
    "#         print(\"LRemb loaded from %s\" % (file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# valid_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = LogisticRegression()\n",
    "# result = test.forward(train_tensor[0:10])\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run LR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_BoW = LogisticRegressionBoW()\n",
    "# model_EmbBoW = LogisticRegressionEmbBoW()\n",
    "# model_Emb = LogisticRegressionEmb()\n",
    "# # print(model.W[:10])\n",
    "# # print(model.linear2.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model BoW param::\n",
      "W <class 'torch.Tensor'> torch.Size([233306, 2])\n",
      "b <class 'torch.Tensor'> torch.Size([2])\n",
      "\n",
      "model EmbBoW param::\n",
      "W <class 'torch.Tensor'> torch.Size([233306])\n",
      "embed.weight <class 'torch.Tensor'> torch.Size([233306, 300])\n",
      "linear1.weight <class 'torch.Tensor'> torch.Size([64, 300])\n",
      "linear1.bias <class 'torch.Tensor'> torch.Size([64])\n",
      "linear2.weight <class 'torch.Tensor'> torch.Size([2, 64])\n",
      "linear2.bias <class 'torch.Tensor'> torch.Size([2])\n",
      "\n",
      "model Emb param::\n",
      "W <class 'torch.Tensor'> torch.Size([300, 1])\n",
      "embed.weight <class 'torch.Tensor'> torch.Size([233306, 300])\n",
      "linear1.weight <class 'torch.Tensor'> torch.Size([64, 300])\n",
      "linear1.bias <class 'torch.Tensor'> torch.Size([64])\n",
      "linear2.weight <class 'torch.Tensor'> torch.Size([2, 64])\n",
      "linear2.bias <class 'torch.Tensor'> torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# print(\"model BoW param::\")\n",
    "# for name, param in model_BoW.named_parameters():\n",
    "#     print(name, type(param.data), param.size())\n",
    "\n",
    "# print(\"\\nmodel EmbBoW param::\")\n",
    "# for name, param in model_EmbBoW.named_parameters():\n",
    "#     print(name, type(param.data), param.size())\n",
    "    \n",
    "# print(\"\\nmodel Emb param::\")\n",
    "# for name, param in model_Emb.named_parameters():\n",
    "#     print(name, type(param.data), param.size())\n",
    "# # list(model.parameters())\n",
    "# # idx = train_tensor[4:5]\n",
    "# # model.W[train_tensor[:4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evalmodel(model):\n",
    "#     test_pred = model.predict(train_data[-valid_num:,:-1])\n",
    "#     difftest = test_pred - torch.squeeze(train_data[-valid_num:,-1])\n",
    "    \n",
    "#     A = torch.sum(test_pred)\n",
    "#     B = torch.sum(train_data[-valid_num:,-1])\n",
    "#     C = torch.sum(torch.abs(difftest))\n",
    "    \n",
    "#     print(\"A: %d\" % (float(A)))\n",
    "#     print(\"B: %d\" % (float(B)))\n",
    "#     print(\"C: %d\" % (float(C)))\n",
    "#     print(\"F1: %.5f\" % (float(A+B-C)/float(A+B)))\n",
    "#     print(\"ACC: %s %s\" % ((1. - float(C)/float(valid_num))*100., \"%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 0 (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Start epoch 1 / 5, \t at Sun Feb 17 20:08:42 2019\n",
      "End of epoch 1 / 5 \t Time taken: 232 sec (or  3 min)\n",
      "Start epoch 2 / 5, \t at Sun Feb 17 20:12:34 2019\n",
      "End of epoch 2 / 5 \t Time taken: 228 sec (or  3 min)\n",
      "Start epoch 3 / 5, \t at Sun Feb 17 20:16:23 2019\n",
      "End of epoch 3 / 5 \t Time taken: 228 sec (or  3 min)\n",
      "Start epoch 4 / 5, \t at Sun Feb 17 20:20:12 2019\n",
      "End of epoch 4 / 5 \t Time taken: 228 sec (or  3 min)\n",
      "Start epoch 5 / 5, \t at Sun Feb 17 20:24:00 2019\n",
      "End of epoch 5 / 5 \t Time taken: 229 sec (or  3 min)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# print(model_BoW.trained)\n",
    "# Train(model_BoW, 6)\n",
    "# print(model_BoW.trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 2825\n",
      "B: 3089\n",
      "C: 4002\n",
      "F1: 0.32330\n",
      "ACC: 91.996 %\n"
     ]
    }
   ],
   "source": [
    "# evalmodel(model_BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 1 (Emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Start epoch 1 / 6, \t at Sun Feb 17 20:39:07 2019\n",
      "End of epoch 1 / 6 \t Time taken: 350 sec (or  5 min)\n",
      "Start epoch 2 / 6, \t at Sun Feb 17 20:44:57 2019\n",
      "End of epoch 2 / 6 \t Time taken: 349 sec (or  5 min)\n",
      "Start epoch 3 / 6, \t at Sun Feb 17 20:50:46 2019\n",
      "End of epoch 3 / 6 \t Time taken: 349 sec (or  5 min)\n",
      "Start epoch 4 / 6, \t at Sun Feb 17 20:56:36 2019\n",
      "End of epoch 4 / 6 \t Time taken: 377 sec (or  6 min)\n",
      "Start epoch 5 / 6, \t at Sun Feb 17 21:02:53 2019\n",
      "End of epoch 5 / 6 \t Time taken: 373 sec (or  6 min)\n",
      "Start epoch 6 / 6, \t at Sun Feb 17 21:09:07 2019\n",
      "LRemb saved at /Users/xyli1905/Projects/NLP/detoxic/checkpoints/net_LRemb_0_id.pth\n",
      "optimizer for LRemb saved at /Users/xyli1905/Projects/NLP/detoxic/checkpoints/opt_LRemb_0_id.pth\n",
      "End of epoch 6 / 6 \t Time taken: 374 sec (or  6 min)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# print(model_Emb.trained)\n",
    "# Train(model_Emb, 6)\n",
    "# print(model_Emb.trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 1759\n",
      "B: 3089\n",
      "C: 2294\n",
      "F1: 0.52682\n",
      "ACC: 95.41199999999999 %\n"
     ]
    }
   ],
   "source": [
    "# evalmodel(model_Emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_Emb.training_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 2 (EmbBoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Start epoch 1 / 6, \t at Sun Feb 17 21:21:13 2019\n",
      "End of epoch 1 / 6 \t Time taken: 276 sec (or  4 min)\n",
      "Start epoch 2 / 6, \t at Sun Feb 17 21:25:49 2019\n",
      "End of epoch 2 / 6 \t Time taken: 269 sec (or  4 min)\n",
      "Start epoch 3 / 6, \t at Sun Feb 17 21:30:19 2019\n",
      "End of epoch 3 / 6 \t Time taken: 269 sec (or  4 min)\n",
      "Start epoch 4 / 6, \t at Sun Feb 17 21:34:48 2019\n",
      "End of epoch 4 / 6 \t Time taken: 295 sec (or  4 min)\n",
      "Start epoch 5 / 6, \t at Sun Feb 17 21:39:44 2019\n",
      "End of epoch 5 / 6 \t Time taken: 295 sec (or  4 min)\n",
      "Start epoch 6 / 6, \t at Sun Feb 17 21:44:39 2019\n",
      "LRembBoW saved at /Users/xyli1905/Projects/NLP/detoxic/checkpoints/net_LRembBoW_0_id.pth\n",
      "optimizer for LRembBoW saved at /Users/xyli1905/Projects/NLP/detoxic/checkpoints/opt_LRembBoW_0_id.pth\n",
      "End of epoch 6 / 6 \t Time taken: 294 sec (or  4 min)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# print(model_EmbBoW.trained)\n",
    "# Train(model_EmbBoW, 6)\n",
    "# print(model_EmbBoW.trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 1803\n",
      "B: 3089\n",
      "C: 2674\n",
      "F1: 0.45339\n",
      "ACC: 94.652 %\n"
     ]
    }
   ],
   "source": [
    "# evalmodel(model_EmbBoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_EmbBoW.training_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define LSTM&GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUmodel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRUmodel, self).__init__()\n",
    "        self.name = \"GRUmodel\"\n",
    "        self.trained = False\n",
    "        self.training_times = 0\n",
    "        self._chkp_dir = ckpt_dir\n",
    "        self._save_mark = 0\n",
    "        \n",
    "        # define parameters\n",
    "        self._vocab_size = vocab_len\n",
    "        self._emb_size = embedding_dim\n",
    "        self._hidden_size = 64\n",
    "        self._output_size = 2 # two labels 0,1\n",
    "        self._dropout_rate = 0.2\n",
    "        ## below for rnn layer\n",
    "        self._number_layers = 1\n",
    "        self._input_size = self._emb_size # 300\n",
    "        self._cell_hidden_size = self._input_size # test\n",
    "        self._cell_dropout = 0 # no dropout in GRU for now\n",
    "        self._bidirectional = False\n",
    "        self._num_dir = 2 if self._bidirectional else 1\n",
    "        self._h_state_vsize = self._number_layers*self._num_dir\n",
    "        \n",
    "        # define layers\n",
    "        self.embed = torch.nn.Embedding(self._vocab_size+1, self._emb_size)\n",
    "        self.dropout = torch.nn.Dropout(self._dropout_rate)\n",
    "#         rnnL = [] # gru layer\n",
    "#         for i in range(self._number_layers):\n",
    "#             inp_size = self._input_size if i == 0 else self._cell_hidden_size\n",
    "#             rnnL.append(torch.nn.GRU(input_size = inp_size, \n",
    "#                                      hidden_size = self._cell_hidden_size, \n",
    "#                                      batch_first = True, # (batch, seqlen, embdim)\n",
    "#                                      dropout = self._cell_dropout,\n",
    "#                                      bidirectional = self._bidirectional\n",
    "#                                     )\n",
    "#                         )\n",
    "#         self.gru = torch.nn.Sequential(*rnnL)\n",
    "        self.gru = torch.nn.GRU(input_size = self._input_size, \n",
    "                                hidden_size = self._cell_hidden_size, \n",
    "                                batch_first = True, # (batch, seqlen, embdim)\n",
    "                                dropout = self._cell_dropout,\n",
    "                                bidirectional = self._bidirectional\n",
    "                               )\n",
    "        self.linear1 = torch.nn.Linear(self._cell_hidden_size, self._hidden_size)\n",
    "        self.linear2 = torch.nn.Linear(self._hidden_size, self._output_size)\n",
    "        \n",
    "        # initialize weights in layser\n",
    "        self.init_weight()\n",
    "    \n",
    "    def init_weight(self):\n",
    "        self.embed.weight.data.copy_(pretrained_weight)\n",
    "        self.embed.weight.requires_grad = False\n",
    "        # with other layers defaultly initialized\n",
    "        \n",
    "    def forward(self, idx_seq):\n",
    "        X = self.embed(idx_seq) # X:(b, seqlen, embdim)\n",
    "        h = Variable(torch.zeros(self._h_state_vsize, idx_seq.shape[0], self._cell_hidden_size))\n",
    "        X, h = self.gru(X, h)\n",
    "        X = self.dropout(X)\n",
    "        X = F.relu(self.linear1(X[:,-1,:]))\n",
    "        out = self.linear2(X).view(-1,self._output_size)\n",
    "        return out\n",
    "        \n",
    "    def predict(self, idx_seq):\n",
    "        '''\n",
    "        0: sincere; 1: toxic\n",
    "        '''\n",
    "        y = F.softmax(self.forward(idx_seq), dim=1)\n",
    "        y_pred = torch.argmax(y, dim=1)\n",
    "        return y_pred\n",
    "    \n",
    "    def save(self):\n",
    "        '''\n",
    "        save trained parameters\n",
    "        '''\n",
    "        fname = 'net_GRUmodel_{}_id.pth'.format(str(self._save_mark))\n",
    "        self._save_mark += 1\n",
    "        save_path = os.path.join(self._chkp_dir, fname)\n",
    "        torch.save(self.state_dict(), save_path)\n",
    "        print(\"GRUmodel saved at %s\" % (save_path))\n",
    "        return self._save_mark - 1\n",
    "        \n",
    "    def load(self, idx=-1):\n",
    "        '''\n",
    "        load trained model with idx,\n",
    "        if idx=-1 load the one with max idx\n",
    "        '''\n",
    "        if idx == -1:\n",
    "            idx_num = 0\n",
    "            for file in os.listdir(self._chkp_dir):\n",
    "                if file.startswith(\"net\"):\n",
    "                    idx_num = max(idx_num, int(file.split('_')[2]))\n",
    "        else:\n",
    "            idx_num = idx\n",
    "        #print(\"loading epoch %d ...\" % epoch_num)\n",
    "        fname = 'net_GRUmodel_{}_id.pth'.format(str(idx_num))\n",
    "        self._save_mark = idx_num\n",
    "        file_path = os.path.join(self._chkp_dir, fname)\n",
    "        self.load_state_dict(torch.load(file_path))\n",
    "        print(\"GRUmodel loaded from %s\" % (file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = GRUmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(test.forward(train_data[:10,:-1]))\n",
    "# for name, param in test.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GRU = GRUmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gru.weight_ih_l0 torch.Size([900, 300])\n",
      "gru.weight_hh_l0 torch.Size([900, 300])\n",
      "gru.bias_ih_l0 torch.Size([900])\n",
      "gru.bias_hh_l0 torch.Size([900])\n",
      "linear1.weight torch.Size([64, 300])\n",
      "linear1.bias torch.Size([64])\n",
      "linear2.weight torch.Size([2, 64])\n",
      "linear2.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_GRU.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_GRU.trained)\n",
    "Train(model_GRU)\n",
    "print(model_GRU.trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class test overwrite/inherit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class basetest:\n",
    "    def __init__(self):\n",
    "        self.name = \"base\"\n",
    "        \n",
    "    def test(self):\n",
    "        print(self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class child(basetest):\n",
    "    def __init__(self):\n",
    "        super(child, self).__init__()\n",
    "        self.name = \"child\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = child()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = torch.nn.Parameter(torch.zeros(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test on model LR classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.LR import BoW, EmbBoW, EmbLR\n",
    "from train import Train\n",
    "import os\n",
    "import pickle\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testEB = EmbBoW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data_proc/processed_data/train_mat.pkl', 'rb') as f:\n",
    "    # train_mat.pkl is np.ndarray\n",
    "    train_data = torch.from_numpy(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W <class 'torch.Tensor'> torch.Size([52309])\n",
      "embed.weight <class 'torch.Tensor'> torch.Size([52309, 300])\n",
      "linear1.weight <class 'torch.Tensor'> torch.Size([64, 300])\n",
      "linear1.bias <class 'torch.Tensor'> torch.Size([64])\n",
      "linear2.weight <class 'torch.Tensor'> torch.Size([2, 64])\n",
      "linear2.bias <class 'torch.Tensor'> torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for name, param in testEB.named_parameters():\n",
    "    print(name, type(param.data), param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1306122, 61])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch 1 / 5, \t at Tue Feb 19 21:42:29 2019\n",
      "End of epoch 1 / 5 \t Time taken: 197 sec (or  3 min)\n",
      "Start epoch 2 / 5, \t at Tue Feb 19 21:45:47 2019\n",
      "Model EmbBoW saved at /Users/xyli1905/Projects/NLP/detoxic/checkpoints/net_EmbBoW_2_id.pth\n",
      "optimizer for EmbBoW saved at /Users/xyli1905/Projects/NLP/detoxic/checkpoints/opt_EmbBoW_2_id.pth\n",
      "End of epoch 2 / 5 \t Time taken: 191 sec (or  3 min)\n",
      "Start epoch 3 / 5, \t at Tue Feb 19 21:48:58 2019\n",
      "End of epoch 3 / 5 \t Time taken: 195 sec (or  3 min)\n",
      "Start epoch 4 / 5, \t at Tue Feb 19 21:52:13 2019\n",
      "Model EmbBoW saved at /Users/xyli1905/Projects/NLP/detoxic/checkpoints/net_EmbBoW_4_id.pth\n",
      "optimizer for EmbBoW saved at /Users/xyli1905/Projects/NLP/detoxic/checkpoints/opt_EmbBoW_4_id.pth\n",
      "End of epoch 4 / 5 \t Time taken: 192 sec (or  3 min)\n",
      "Start epoch 5 / 5, \t at Tue Feb 19 21:55:25 2019\n",
      "End of epoch 5 / 5 \t Time taken: 189 sec (or  3 min)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<train.Train at 0x1293f32b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_num = 100000\n",
    "Train(model=testEB, train_data=train_data[:-valid_num,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalmodel(model):\n",
    "    test_pred = model.predict(train_data[-valid_num:,:-1])\n",
    "    difftest = test_pred - torch.squeeze(train_data[-valid_num:,-1])\n",
    "    \n",
    "    A = torch.sum(test_pred)\n",
    "    B = torch.sum(train_data[-valid_num:,-1])\n",
    "    C = torch.sum(torch.abs(difftest))\n",
    "    \n",
    "    print(\"A: %d\" % (float(A)))\n",
    "    print(\"B: %d\" % (float(B)))\n",
    "    print(\"C: %d\" % (float(C)))\n",
    "    print(\"F1: %.5f\" % (float(A+B-C)/float(A+B)))\n",
    "    print(\"ACC: %s %s\" % ((1. - float(C)/float(valid_num))*100., \"%\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 2913\n",
      "B: 6301\n",
      "C: 5492\n",
      "F1: 0.40395\n",
      "ACC: 94.50800000000001 %\n"
     ]
    }
   ],
   "source": [
    "evalmodel(testEB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
